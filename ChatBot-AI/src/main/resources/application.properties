spring.application.name=streaming-ai
spring.ai.ollama.chat.options.model=llama3.2:3b
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.keep-alive=30m
spring.ai.ollama.chat.options.temperature=0.7
server.port=8081
logging.level.dev.jor.streaming_ai=DEBUG
